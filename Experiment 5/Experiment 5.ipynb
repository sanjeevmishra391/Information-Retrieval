{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "\n",
    "# del doc_word_mapper\n",
    "gc.collect() # in case if ram keeps increasing\n",
    "\n",
    "\n",
    "label_count_mapper = np.zeros((20, 20))\n",
    "class_docs = {} # how much docs each class has\n",
    "train_y = [] # training labels (1-20)\n",
    "conf_matrix = np.zeros((20, 20))\n",
    "\n",
    "with open('./news_data/train.label', 'r') as fp:\n",
    "  for line in fp:\n",
    "    label = int(line.strip())\n",
    "    train_y.append(label)\n",
    "    class_docs[label] = class_docs.get(label, 0) + 1\n",
    "    \n",
    "previous_doc = 1\n",
    "freq = {} # (word, class): count\n",
    "with open('./news_data/train.data', 'r') as fp:\n",
    "  word_ids = [] # this will basically contain the words and their counts\n",
    "  for line in fp:\n",
    "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
    "    if previous_doc != doc_id:\n",
    "      # doc N will have label at (N-1)th place\n",
    "      class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
    "      for word, word_count in word_ids:\n",
    "        freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
    "      previous_doc = doc_id\n",
    "      word_ids = [(word_id, count)]\n",
    "    else:\n",
    "      word_ids.append((word_id, count)) # appending word_id\n",
    "# for the last doc\n",
    "class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
    "for word, count in word_ids:\n",
    "  freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
    "del word_ids\n",
    "\n",
    "vocab = set([pair[0] for pair in freq.keys()])\n",
    "v_len = len(vocab)\n",
    "\n",
    "# finding how much words each class has\n",
    "class_words = {}\n",
    "for pair, word_count in freq.items():\n",
    "  class_ = pair[1]\n",
    "  class_words[class_] = class_words.get(class_, 0) + word_count\n",
    "\n",
    "total_docs = len(train_y) # this should be 11269 if using their indexing (see train.data)\n",
    "# sum(class_docs.values()) # again, this should be same as total_docs\n",
    "\n",
    "prob_class = {}\n",
    "prob_word_class = {}\n",
    "\n",
    "# finding probability of each class\n",
    "for i in class_docs:\n",
    "  prob_class[i] = class_docs[i]/total_docs\n",
    "\n",
    "# findinf probability of each word in each class\n",
    "# we are doing this with smoothing too, for better results\n",
    "for word in vocab:\n",
    "  for class_ in class_words:\n",
    "    freq_class = freq.get((word, class_), 0)\n",
    "    # word/class\n",
    "    prob_word_class[(word, class_)] = (freq_class + 1)/(class_words[class_] + v_len)\n",
    "    \n",
    "    \n",
    "    \n",
    "conf_matrix = np.zeros((20,20))\n",
    "previous_doc = 1\n",
    "y_expected = []\n",
    "y_actual = []\n",
    "with open('./news_data/test.label', 'r') as fp:\n",
    "  for line in fp:\n",
    "    y_expected.append(int(line.strip()))\n",
    "total_test_docs = len(y_expected)\n",
    "correct_classified = 0\n",
    "with open('./news_data/test.data', 'r') as fp:\n",
    "  word_ids = [] # this will basically contain the words and their counts\n",
    "  j = 0\n",
    "  for line in fp:\n",
    "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
    "    if previous_doc != doc_id:\n",
    "      probs = deepcopy(prob_class)\n",
    "      for i in probs:\n",
    "        probs[i] = np.log(probs[i])\n",
    "      for word, word_count in word_ids:\n",
    "        for class_ in range(1,21):\n",
    "          # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
    "          probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
    "          # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
    "      _max_class = 1\n",
    "      _max_val = - np.inf\n",
    "      for i in probs:\n",
    "        if probs[i] > _max_val:\n",
    "          _max_val = probs[i]\n",
    "          _max_class = i\n",
    "      y_actual.append(_max_class)\n",
    "      # print(_max_class, end = ' ')\n",
    "      if y_expected[j] == _max_class:\n",
    "        correct_classified+=1\n",
    "      conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
    "      j += 1\n",
    "      previous_doc = doc_id\n",
    "      word_ids = [(word_id, count)]\n",
    "    else:\n",
    "      word_ids.append((word_id, count)) # appending word_id\n",
    "\n",
    "# this is for the last word_ids (code can be taken into function to remove redunduncy)\n",
    "for word, word_count in word_ids:\n",
    "  for class_ in range(1,21):\n",
    "    # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
    "    probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
    "    # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
    "_max_class = 1\n",
    "_max_val = - np.inf\n",
    "for i in probs:\n",
    "  if probs[i] > _max_val:\n",
    "    _max_val = probs[i]\n",
    "    _max_class = i\n",
    "# print(_max_class, end = ' ')\n",
    "y_actual.append(_max_class)\n",
    "if y_expected[j] == _max_class:\n",
    "  correct_classified+=1\n",
    "conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
    "\n",
    "incorrect_classified = total_test_docs - correct_classified\n",
    "print(correct_classified/total_test_docs)\n",
    "\n",
    "\n",
    "# # FOR F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_expected, y_actual, average=None)\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"Let me take you down\n",
    "'Cause I'm going to Strawberry Fields\n",
    "Nothing is real\n",
    "And nothing to get hung about\n",
    "Strawberry Fields forever\n",
    "Living is easy with eyes closed\n",
    "Misunderstanding all you see\n",
    "It's getting hard to be someone\n",
    "But it all works out\n",
    "It doesn't matter much to me\n",
    "Let me take you down\n",
    "'Cause I'm going to Strawberry Fields\n",
    "Nothing is real\n",
    "And nothing to get hung about\n",
    "Strawberry Fields forever\n",
    "No one I think is in my tree\n",
    "I mean it must be high or low\n",
    "That is you can't, you know, tune in\n",
    "But it's all right\n",
    "That is I think it's not too bad\n",
    "Let me take you down\n",
    "'Cause I'm going to Strawberry Fields\n",
    "Nothing is real\n",
    "And nothing to get hung about\n",
    "Strawberry Fields forever\n",
    "Always, no sometimes, think it's me\n",
    "But you know I know when it's a dream\n",
    "I think I know I mean a yes\n",
    "But it's all wrong\n",
    "That is I think I disagree\n",
    "Let me take you down\n",
    "'Cause I'm going to Strawberry Fields\n",
    "Nothing is real\n",
    "And nothing to get hung about\n",
    "Strawberry Fields forever\n",
    "Strawberry Fields forever\n",
    "Strawberry Fields forever\"\"\"\n",
    "\n",
    "def get_index(char: str) -> int:\n",
    "    if char.isalpha():\n",
    "      return ord(char.lower()) - 97 # so a comes on 0, b-1, ...z-25\n",
    "    else:\n",
    "      return 26\n",
    "\n",
    "def get_occurence_matrix(text):\n",
    "  occurence_matrix = np.zeros((27,27))\n",
    "  l = len(text)\n",
    "  # a => 97, z => 122\n",
    "  for i in range(1, l):\n",
    "    current = text[i-1]\n",
    "    next = text[i]\n",
    "    current_index = get_index(current)\n",
    "    next_index = get_index(next)\n",
    "    occurence_matrix[current_index, next_index] += 1\n",
    "  return occurence_matrix\n",
    "\n",
    "\n",
    "\n",
    "occurence_matrix = get_occurence_matrix(text)\n",
    "def get_probability_matrix(occurence_matrix):\n",
    "  prob_matrix =  occurence_matrix/occurence_matrix.sum(axis=1, keepdims=True)\n",
    "  prob_matrix[np.isnan(prob_matrix)] = 0 # removing NaNs\n",
    "  return prob_matrix\n",
    "get_probability_matrix(occurence_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
