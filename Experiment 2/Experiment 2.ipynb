{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import heapq\n",
    "import json\n",
    "import gc\n",
    "porter = PorterStemmer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# used to remove symbols from the text\n",
    "def remove_symbols(line):\n",
    "    return re.sub('[^A-Za-z0-9\\s]+', '', line).lower()\n",
    "\n",
    "# we are using this to change list into set while dumping json into file\n",
    "class SetEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "    if isinstance(obj, set):\n",
    "      return list(obj)\n",
    "    return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize) # if we don't do this, we won't be able to read whole line\n",
    "\n",
    "\n",
    "# bsbi algorithm\n",
    "def bsbi():\n",
    "  freq_dict = defaultdict(set)\n",
    "  with open('gutenberg_data.csv') as f:\n",
    "    next(f) # just skipping first line(column headers)\n",
    "    csv_file = csv.reader(f)\n",
    "    total_files = 0\n",
    "    i = 0\n",
    "    current_block = 0\n",
    "    for line in csv_file:\n",
    "      title, author, link, id, bookshelf, text = line\n",
    "      # print(id,\" => \" ,author)\n",
    "      i += 1\n",
    "      # for testing \n",
    "      # if i == 2:\n",
    "      #   break\n",
    "      for word in text.split():\n",
    "        word = remove_symbols(word)\n",
    "        if word and word not in stop_words:\n",
    "          word = porter.stem(word)\n",
    "          if word not in freq_dict:\n",
    "            # if word is not added before, we will increase block size too\n",
    "            current_block += 1\n",
    "          \"\"\"\n",
    "          note: __contains__ will just check if doc is already there.\n",
    "          It's not exactly needed as we are using set, but we are checking it to manage block size\n",
    "          also searching in sets is faster as they are hashed (while lists are not hashed)\n",
    "          \"\"\"\n",
    "          if not freq_dict[word].__contains__(id):\n",
    "            freq_dict[word].add(id)\n",
    "            current_block += 1\n",
    "        if current_block >= BLOCK_SIZE:\n",
    "          # LETS DO THE WRITE OPERATION\n",
    "          sorted_list = sorted(freq_dict.items(), key=lambda _: _[0]) # sorting by word_id\n",
    "          with open(f'./IROUTPUTS_v2/OP{total_files}.txt', 'w') as  f:\n",
    "            # json.dump(freq_dict, f, cls=SetEncoder)\n",
    "            for word_id, docs in sorted_list:\n",
    "              f.write(word_id)\n",
    "              for doc_id in docs:\n",
    "                f.write(f' {doc_id}')\n",
    "              f.write('\\n')\n",
    "          current_block = 0\n",
    "          freq_dict.clear()\n",
    "          total_files += 1\n",
    "          print(i, ' rows done')\n",
    "          # if total_files == 2:\n",
    "          #   return\n",
    "      \n",
    "    sorted_list = sorted(freq_dict.items(), key=lambda _: _[0]) # sorting by word_id\n",
    "    # this is for last values\n",
    "    # TODO:: DO IT BY FN SO NO REPEATATION OF CODE\n",
    "    if len(sorted_list) > 0:\n",
    "      with open(f'./IROUTPUTS_v2/OP{total_files}.txt', 'w') as  f:\n",
    "        # json.dump(freq_dict, f, cls=SetEncoder)\n",
    "        for word_id, docs in sorted_list:\n",
    "          f.write(word_id)\n",
    "          for doc_id in docs:\n",
    "            f.write(f' {doc_id}')\n",
    "          f.write('\\n')\n",
    "      current_block = 0\n",
    "      freq_dict.clear()\n",
    "      total_files += 1\n",
    "bsbi()\n",
    "\n",
    "\n",
    "file_names = [f'./IROUTPUTS/OP{i}.txt' for i in range(947)]\n",
    "file_pointers = [open(i) for i in file_names]\n",
    "\n",
    "\n",
    "# here we are using yeild so we are JUST READING ONE LINE at a time\n",
    "# you can read more about it by searching generators in python\n",
    "def decorated_file(f, key):\n",
    "  for line in f:\n",
    "    yield (key(line), line)\n",
    "\n",
    "files = map(open, file_names)\n",
    "outfile = open('./IROUTPUTS_v2/merged.txt', 'w')\n",
    "\n",
    "def key_fn(line):\n",
    "    return line.split(' ', 2)[0] # returning word_id\n",
    "\n",
    "\n",
    "prev = ''\n",
    "for line in heapq.merge(*[decorated_file(f, key_fn) for f in files]):\n",
    "  # to understand this, you can do this over small number of sorted files and try to print line\n",
    "  if prev != line[0]:\n",
    "    # if we have new word, make sure to add new line at first\n",
    "    outfile.write(f'\\n{line[1].strip()}')\n",
    "    prev = line[0]\n",
    "  # if we have same word yet, put a space and add other ids\n",
    "  else:\n",
    "    # line[1][len(line[0]):] => We are removing the word_id string and then writing the line\n",
    "    outfile.write(f' {line[1][len(line[0]):].strip()}')\n",
    "for i in file_pointers:\n",
    "  i.close()\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
